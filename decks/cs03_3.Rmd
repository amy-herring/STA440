---
title: "Multilevel Regression and Poststratification"
output: 
  revealjs::revealjs_presentation:
    theme: night
    highlight: espresso
    center: true
    transition: none
    fig_caption: true
    reveal_options:
      progress: true
      slideNumber: true
---

```{r setupa, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lme4)
library(brms) #simple interface with Stan
library(rstan)
#devtools::install_github("hrbrmstr/albersusa")
library(albersusa) #plotting -- amy could not install this time around
library(cowplot) # plotting
library(dplyr)
library(directlabels)
library(tidybayes) #work easily with posterior samples
rstan_options(auto_write=TRUE)
options(mc.cores=parallel::detectCores())
```

## Introduction

- It is often of interest to researchers to consider state-level opinion, in addition to/instead of national-level opinion. 
- Finding surveys that are uniform across all or most states is extremely challenging, and states done for one state sometimes are of lower quality than national-level surveys. 
- One method of estimating state-level opinion using national survey data is called **multilevel regression and poststratification ("Mr. P")**. 
- We will compare this approach with a simple approach of using empirical means and poststratifying without borrowing information across groups.

## Load packages


```{r setup, eval=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lme4)
library(brms) #simple interface with Stan
library(rstan)
#devtools::install_github("hrbrmstr/albersusa")
library(albersusa) #plotting
library(cowplot) # plotting
library(directlabels)
library(tidybayes) #work easily with posterior samples
library(dplyr)
rstan_options(auto_write=TRUE)
options(mc.cores=parallel::detectCores())
```


## Multilevel Modeling with Poststratification

First, we use multilevel regression to model individual survey responses as a function of demographic and geographic predictors. 

Then we use poststratification, in which we weight (poststratify) the estimates for each demographic-geographic respondent type by the percentages of each type in the actual state populations.

## Tutorial

This tutorial draws heavily on Jonathan Kastellec's [MrP primer](http://www.princeton.edu/~jkastell/mrp_primer.html) and [Tim Mastny's version using Stan](https://timmastny.rbind.io/blog/multilevel-mrp-tidybayes-brms-stan/). You may find the paper at their website useful in addition to the shorter version presented here. First, download all three datasets [at this link](http://www.princeton.edu/~jkastell/mrp_primer.html).

## Analysis goal

The goal is to estimate support for gay marriage in each state based on survey data that are potentially non-representative. Because not all subgroups of the population are equally likely to respond to polls, we worry that relying only on a survey could lead to biased estimates of support for gay marriage.  (For example, younger people may be more likely than older people to answer questions about gay marriage.)

The US Census is a good source of information about characteristics of the full US population. Using Census data, we can scale the average support of population subgorups of interest in proportion to their representation in the state population.

## Data

- A compilation of national gay marriage polls will provide information on support of gay marriage. Five national polls were conducted in 2004 and include information on state, gender, race/ethnicity, education, age, and party identification.
- State level data provide information including \% of religious voters, voting history (Democrat or Republican), etc.
- Census data will be used to estimate the \% of voters in subgroups in the state, given that poll respondents may not mirror the demographics of voting-age citizens. Ultimately, we need a dataset of the population counts for each subgroup, e.g. how many African Americans aged 18-25 went to college and reside in NC. For this tutorial, we will use the 5% Public Use Microdata Sample from the 2000 census. 

##


```{r getdata, cache=TRUE}
marriage.data=foreign::read.dta('data/gay_marriage_megapoll.dta',
                                convert.underscore=TRUE)
marriage.data$state=as.factor(marriage.data$state)
marriage.data$region=as.factor(marriage.data$region)
Statelevel=foreign::read.dta('data/state_level_update.dta',
                             convert.underscore=TRUE)
Census=foreign::read.dta('data/poststratification 2000.dta',
                         convert.underscore=TRUE)
```


## Data munging

```{r datamanipulation, cache=TRUE}
Statelevel=Statelevel %>% rename(state=sstate) #rename to state
marriage.data=Statelevel %>%
  select(state,p.evang,p.mormon,kerry.04) %>%
  right_join(marriage.data)
```

## Data munging

```{r datamanipulation2, cache=TRUE}
# combine demographic groups and label them
marriage.data$race.female <- (marriage.data$female *3) + marriage.data$race.wbh
marriage.data$race.female=factor(marriage.data$race.female,levels=1:6,
                                 labels=c("WhMale","BlMale","HMale","WhFem","BlFem","HFem"))
marriage.data$age.edu.cat <- 4 * (marriage.data$age.cat -1) + marriage.data$edu.cat
marriage.data$age.edu.cat=factor(marriage.data$age.edu.cat,levels=1:16,
                                 labels=c("18-29,<HS","18-29,HS","18-29,SC","18-29,CG",
                                          "30-44,<HS","30-44,HS","30-44,SC","30-44,CG",
                                          "45-64,<HS","45-64,HS","45-64,SC","45-64,CG",
                                          "65+,<HS","65+,HS","65+,SC","65+,CG"))
marriage.data$p.evang <- Statelevel$p.evang[marriage.data$state.initnum]
# proportion of evangelicals in respondent's state
marriage.data$p.mormon <-Statelevel$p.mormon[marriage.data$state.initnum]
# proportion of LDS church members in respondent's state
marriage.data$p.relig <- marriage.data$p.evang + marriage.data$p.mormon
# combined evangelical + LDS proportions
marriage.data$kerry.04 <- Statelevel$kerry.04[marriage.data$state.initnum]
# John Kerry's % of 2-party vote in respondent's state in 2004
marriage.data <- marriage.data %>%
  filter(state!="")
```

## Data manipulation

```{r datamanipulation3, cache=TRUE}
# Census variables
Census<-Census %>%
  rename(state=cstate, age.cat=cage.cat, edu.cat=cedu.cat,region=cregion)
Census$race.female <- (Census$cfemale *3) + Census$crace.WBH 
Census$race.female=factor(Census$race.female,levels=1:6,
                           labels=c("WhMale","BlMale","HMale","WhFem","BlFem","HFem"))
Census$age.edu.cat <- 4 * (Census$age.cat-1) + Census$edu.cat 
Census$age.edu.cat=factor(Census$age.edu.cat,levels=1:16,
                           labels=c("18-29,<HS","18-29,HS","18-29,SC","18-29,CG",
                                    "30-44,<HS","30-44,HS","30-44,SC","30-44,CG",
                                    "45-64,<HS","45-64,HS","45-64,SC","45-64,CG",
                                    "65+,<HS","65+,HS","65+,SC","65+,CG"))
Census <- Statelevel %>%
  select(state,p.evang,p.mormon,kerry.04) %>%
  right_join(Census)
Census <- Census %>% mutate(p.relig=p.evang+p.mormon)
```

## Obtain estimates based on empirical averages

We'll call these averages a disaggregate model -- just taking the mean responses within each state. 

```{r empirical, cache=TRUE}
# Get state averages
mod.disag=marriage.data%>%
  group_by(state) %>%
  summarise(support=mean(yes.of.all)) %>%
  mutate(model="no_ps")
```

These averages will not be representative of the actual statewide means if the sampled respondents are not in proportion to each group's percentage of the total state population. So we'll next poststratify.

## Poststratifying sample estimates

First, we find within-group averages in each state.

```{r groupmeans, cache=TRUE}
# Find average within each group
grp.means<-marriage.data%>% 
  group_by(state,region,race.female,age.edu.cat,p.relig,kerry.04) %>%
  summarize(support=mean(yes.of.all,na.rm=TRUE))
```

## Poststratifying sample estimates

Next we add the group's percentage in each state.

```{r grouppct, cache=TRUE}
grp.means<- Census %>%
  select(state, region, kerry.04, race.female, age.edu.cat, p.relig, 
         cpercent.state) %>%
  right_join(grp.means)
```

## Poststratifying sample estimates

```{r poststratempirical, cache=TRUE}
# sum scaled average and get total state averages
mod.disag.ps<-grp.means %>%
  group_by(state) %>%
  summarize(support=sum(support * cpercent.state, na.rm=TRUE)) %>%
  mutate(model="ps")
```



## Plotting empirical and poststratified means

```{r makeplot, eval=FALSE}
#compare poststratified and empirical means -- nice plot!
disag.point <- bind_rows(mod.disag,mod.disag.ps) %>%
  group_by(model) %>%
  arrange(support, .by_group=TRUE) %>%
  ggplot(aes(x=support,y=forcats::fct_inorder(state),color=model)) +
  geom_point() + theme_classic() +
  theme(legend.position='none') +
  directlabels::geom_dl(aes(label=model),method='smart.grid') +
  ylab('state')
disag.point

```

## Plots

```{r showplot, echo=FALSE, cache=TRUE, out.width = '700px', dpi=200}
disag.point <- bind_rows(mod.disag,mod.disag.ps) %>%
  group_by(model) %>%
  arrange(support, .by_group=TRUE) %>%
  ggplot(aes(x=support,y=forcats::fct_inorder(state),color=model)) +
  geom_point() + theme_classic() +
  theme(legend.position='none') +
  directlabels::geom_dl(aes(label=model),method='smart.grid') +
  ylab('state')
disag.point
```

<small>
Variation in poststratified estimates is pretty large. Also, the poststratified estimates appear closer to zero -- what is going on?
</small>

## Demographic representation by state

Let's sum the percentages in the individual marriage data by state to make sure they all sum to 1.

```{r demogrep, eval=FALSE, cache=TRUE}
grp.means %>%
  group_by(state) %>%
  summarize(total_percent=sum(cpercent.state, na.rm=TRUE)) %>%
  filter(state != "") %>%
  ggplot(aes(x=state,y=total_percent)) +
  geom_text(aes(label=state),hjust=0.5,vjust=0.25) +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  coord_fixed(ratio=8)+ylim(c(0,1.1))
```

## Demographic representation by state

```{r demogrepb, echo=FALSE, cache=TRUE, out.width = '550px', dpi=200}
grp.means %>%
  group_by(state) %>%
  summarize(total_percent=sum(cpercent.state, na.rm=TRUE)) %>%
  filter(state != "") %>%
  ggplot(aes(x=state,y=total_percent)) +
  geom_text(aes(label=state),hjust=0.5,vjust=0.25) +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  coord_fixed(ratio=8)+ylim(c(0,1.1))
```

<small>
Ahh, the surveys do not have responses from each demographic group in each state. Our poststratification is assuming the missing demographic groups have 0% support, which 
is not good -- even though we have no black men from South Dakota in the survey, 
there are some in the state (1.7\% of the SD population identifies as black or African-American). We underestimate the level of support by assuming
no black men in SD support gay marriage.
</small>

## Multilevel model

One advantage of fitting a multilevel model is that we can borrow information to get better estimates.

In the case of African-American men from South Dakota, we do have responses from black men in nearby states (North Dakota has roughly 3x the African-American population of South Dakota) and other US states, which we can use to make a better guess about the level of support for gay marriage among black men in South Dakota.

## Fitting Individual-level Regression Model

Now we  fit a regression model for individual survey responses on gay marriage rights given demographics and geography, i.e. each individual's response will be a function of their demographics and state. 

We'll denote each individual as *i* with indices for race-gender combination, age-education combination, region, and state. We let \[y_i=1\] for supporters of same-sex marriage and 0 for opponents and those with no opinion. 

## Model

\[
\mathrm{logit}\left(\mathrm{Pr}(y_i = 1)\right) = \beta_0 + \alpha^{race,gender}_{j[i]} +  \alpha^{age,edu}_{k[i]} + \alpha^{state}_{s[i]}
\]

We can think of the terms after the intercept as modeled effects for different groups of respondents such as individuals who are a specific age. All of them except the state coefficient will be modeled as drawn from a normal distribution with mean zero and some estimated variance specific to that variable. For example,

\begin{eqnarray*}
\alpha^{race,gender}_j &\sim& N(0, \sigma^2_{race,gender}), ~~~\mathrm{j=1, ..., 6} \\
\alpha^{age,edu}_k &\sim& N(0, \sigma^2_{age,edu}), ~~~\mathrm{k=1,...,16} 
\end{eqnarray*}


## Model

For the state effect, we model the mean for the state effect as a function of 3 state level variables: the region into which the state falls, the state’s conservative (defined as evangelical+LDS) religious percentage, and its Democratic 2004 presidential vote share.

$$\alpha^{state}_s \sim N(\alpha^{reg}_{m[s]} + \beta^{relig}\cdot relig_s + \beta^{vote} \cdot vote_s, \sigma^2_{state}),$$ $\mathrm{s = 1, ..., 51}$,
$$\alpha^{reg}_m \sim N(0,\sigma^2_{region})$$



## Model coding


```{r indiv1, cache=TRUE}
#run individual-level opinion model
ml.mod <- glmer(formula = yes.of.all ~ (1|race.female)
                    + (1|age.cat) + (1|edu.cat) + (1|age.edu.cat)
                    + (1|state) + (1|region) + 
                    + p.relig + kerry.04,
                    data=marriage.data, family=binomial(link="logit"))
# Note: (1|variable) denotes a random effect
```

## Model results

```{r indiv1b}
summary(ml.mod)
```

## Model results

Note we have no responses from AK or HI.

```{r AKHI}
# note nobody from AK or HI in survey
marriage.data %>%
  filter(state=="AK", state=="HI")
```

## Predictions 
Using the predict package, we make predictions in states, broken out by the demographic groups of interest, which will allow us to poststratify down the road. For now we calculate the predictions, and we'll examine them closely later.

```{r predm1, cache=TRUE}
ps.ml.mod <- Census %>%
  mutate(support=predict(ml.mod,newdata=.,allow.new.levels=TRUE,type='response')) %>%
  mutate(support=support*cpercent.state) %>%
  group_by(state) %>%
  summarize(support=sum(support))
```

## Bayesian model

Now we fit a fully Bayesian model, with same data model as the ML model but with some weakly informative priors on the SD's of varying intercepts that will help with borrowing of information and convergence.

```{r bayesmod, eval=FALSE}
bayes.mod <- brm(yes.of.all ~ (1|race.female) + (1|age.cat) + (1|edu.cat)
                 + (1|age.edu.cat) + (1|state) + (1|region)
                 + p.relig + kerry.04,
                 data=marriage.data, family=bernoulli(),
                 prior=c(set_prior("normal(0,0.2)", class='b'),
                         set_prior("normal(0,0.2)", class='sd', group="race.female"),
                         set_prior("normal(0,0.2)", class='sd', group="age.cat"),
                         set_prior("normal(0,0.2)", class='sd', group="edu.cat"),
                         set_prior("normal(0,0.2)", class='sd', group="age.edu.cat"),
                         set_prior("normal(0,0.2)", class='sd', group="state"),
                         set_prior("normal(0,0.2)", class='sd', group="region")))
```

## Bayesian model 

```{r bayesmod2, echo=FALSE, cache=TRUE}
bayes.mod <- brm(yes.of.all ~ (1|race.female) + (1|age.cat) + (1|edu.cat)
                 + (1|age.edu.cat) + (1|state) + (1|region)
                 + p.relig + kerry.04,
                 data=marriage.data, family=bernoulli(),
                 prior=c(set_prior("normal(0,0.2)", class='b'),
                         set_prior("normal(0,0.2)", class='sd', group="race.female"),
                         set_prior("normal(0,0.2)", class='sd', group="age.cat"),
                         set_prior("normal(0,0.2)", class='sd', group="edu.cat"),
                         set_prior("normal(0,0.2)", class='sd', group="age.edu.cat"),
                         set_prior("normal(0,0.2)", class='sd', group="state"),
                         set_prior("normal(0,0.2)", class='sd', group="region")))
```

## Bayesian model results

```{r bayesmod3}
summary(bayes.mod)
```

## Benefits of Bayesian approach

The most obvious benefit of a Bayesian approach is the total accounting of uncertainty, as we can easily see by comparing the estimated SD's of the group-level intercepts in the frequentist model to the posteriors from the Bayesian model. 

```{r bayesbenefits, eval=FALSE}
bayes.mod %>%
  gather_samples(`sd.*`,regex=TRUE) %>%
  ungroup() %>%
  mutate(group=stringr::str_replace_all(term,c("sd_"="","__Intercept"=""))) %>%
  ggplot(aes(y=group,x=estimate)) +
  ggridges::geom_density_ridges(aes(height=..density..),
                                rel_min_height=0.01,stat="density",
                                scale=1.5) 
```

## Benefits of Bayesian approach

Recall the sd estimates from the frequentist model were 0.06 for state, 0.22 for region, 0.23 for race.female, 0.32 for edu.cat,0.10 for age.edu.cat, and 0.41 for age.cat.

```{r bayesbenefits2, echo=FALSE, cache=TRUE, warnings=FALSE, out.width = '380px', dpi=200}
bayes.mod %>%
  gather_samples(`sd.*`,regex=TRUE) %>%
  ungroup() %>%
  mutate(group=stringr::str_replace_all(term,c("sd_"="","__Intercept"=""))) %>%
  ggplot(aes(y=group,x=estimate)) +
  ggridges::geom_density_ridges(aes(height=..density..),
                                rel_min_height=0.01,stat="density",
                                scale=1.5) 
```
<small>
Wow, this is cool!  The Bayesian model gives you an idea of the full posterior distribution of values, from which we can sample, as opposed to a single point estimate. 
</small>

## Poststratifying Bayes

```{r postbayes, cache=TRUE}
#next let's get the point estimate and poststratify from the Bayesian model
ps.bayes.mod <- bayes.mod %>%
  add_predicted_samples(newdata=Census, allow_new_levels=TRUE) %>%
  rename(support = pred) %>%
  mean_qi() %>%
  mutate(support = support * cpercent.state) %>%
  group_by(state) %>%
  summarize(support = sum(support))
```

## Comparing results
Now we consider comparisons across the 3 approaches.
```{r compareresults, cache=TRUE, eval=FALSE}
mod.disag[nrow(mod.disag) + 1,] = list("AK", mean(mod.disag$support), "no_ps")
mod.disag[nrow(mod.disag) + 1,] = list("HI", mean(mod.disag$support), "no_ps")

test=full_join(mod.disag,ps.ml.mod,by=c("state"),suffix=c("_disag","_ml"))
test2=full_join(test,ps.bayes.mod,by=c("state"))
test2$support_bayes=test2$support

ggplot(data=test2, aes(x = support_bayes, y=support_ml)) +
    geom_text(aes(label=state), hjust=0.5, vjust=0.25) +
    geom_abline(slope = 1, intercept = 0) +
    xlim(c(0,0.7)) + ylim(c(0,0.7)) + 
    xlab("Bayes poststrat support") + ylab("ML poststrat support") +
    coord_fixed()

ggplot(data=test2, aes(x = support_bayes, y=support_disag)) +
    geom_text(aes(label=state), hjust=0.5, vjust=0.25) +
    geom_abline(slope = 1, intercept = 0) +
    xlim(c(0,0.7)) + ylim(c(0,0.7)) + 
    xlab("Bayes poststrat support") + ylab("Unstratified support") +
    coord_fixed()

```



## Comparing results
Now we consider comparisons across the 3 approaches.
```{r compareresults2, cache=TRUE, echo=FALSE}
mod.disag[nrow(mod.disag) + 1,] = list("AK", mean(mod.disag$support), "no_ps")
mod.disag[nrow(mod.disag) + 1,] = list("HI", mean(mod.disag$support), "no_ps")

test=full_join(mod.disag,ps.ml.mod,by=c("state"),suffix=c("_disag","_ml"))
test2=full_join(test,ps.bayes.mod,by=c("state"))
test2$support_bayes=test2$support

ggplot(data=test2, aes(x = support_bayes, y=support_ml)) +
    geom_text(aes(label=state), hjust=0.5, vjust=0.25) +
    geom_abline(slope = 1, intercept = 0) +
    xlim(c(0,0.7)) + ylim(c(0,0.7)) + 
    xlab("Bayes poststrat support") + ylab("ML poststrat support") +
    coord_fixed()
```

## Comparisons

```{r compareresults3, cache=TRUE, echo=FALSE}
ggplot(data=test2, aes(x = support_bayes, y=support_disag)) +
    geom_text(aes(label=state), hjust=0.5, vjust=0.25) +
    geom_abline(slope = 1, intercept = 0) +
    xlim(c(0,0.7)) + ylim(c(0,0.7)) + 
    xlab("Bayes poststrat support") + ylab("Unstratified support") +
    coord_fixed()

```




## Comparisons

Note our predicted probabilities from the ML and Bayesian approaches are similar, despite having different parameter estimates, and the models disagree with the disaggregated model, which does not borrow information.


## Prediction

Now we can evaluate predictions, taking advantage of the uncertainty quantification advantages of the Bayesian approach. We will sample from the posterior to get predicted probabilities for each group of interest based on proportions obtained from
the Census data.

```{r predict, cache=TRUE}
predict_val=predict(bayes.mod, newdata=Census, allow_new_levels=TRUE, 
            nsamples=500, summary=FALSE)
```


## Prediction

```{r showpred}
dim(Census)
head(Census)
```


## Sampling

We can generate individual samples from our Bayesian model's predicted probabilities and use them to estimate public opinion under a variety of assumptions (e.g., estimating opinion of all residents, or just of likely voters).

## Sampling

```{r showpred2}
options(width = 10000)
str(predict(bayes.mod, newdata=Census, allow_new_levels=TRUE, 
        nsamples=500, summary=FALSE))
# add predictions to our data
pb=bayes.mod %>%
  add_predicted_draws(newdata=Census, allow_new_levels=TRUE, n=500) 
```


